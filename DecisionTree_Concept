# 决策树基础

## 一. 熵
###1.1 信息量
首先我们考虑一下如何度量信息量.
显然,信息量的大小跟事情不确定性的变化有关,而不确定性的变化和事情可能结果的熟练给有关; 和每个结果的发生概率有关.
我们把信息量的特点总结如下:

* 非负性: 不然说句话还偷走信息~
* 可加性: 对于独立事件发生的信息量是可加的
* 依赖于概率: 事件发生的概率越小,信息量越大,确定的事件(如:太阳东方升起)信息量为0.

满足以上三个特点的信息量函数公式如下:
$$I_X(x)=-log[P_X(x)]=log(\frac{1}{p_X(x)})$$

**信息熵就是平均而言发生一个事件我们得到的信息量大小。信息熵其实是信息量的期望**
$$H(x)=\sum_{x}-p_X(x)log[p_X(x)]=E[I_X(x)]$$

### 1.2 信息熵
Shannon认为信息熵应有三大基本属性：
1. 信息熵应该是随概率连续变化的
2. 当所有可能选择的概率Pi相等时（即1/n）,信息熵随总数n的增加而单调增加。即事件的可能选择越多，不确定性越大。
3. 当一个选择，可以分解为两个连续选择时。分解前后的熵值应该相等，不确定性相同。

当事件为互斥事件时，其中结果1的概率为p,结果2的概率为1-p, 则信息熵随p变化的函数图像为:
![avatar](https://github.com/superfishie/img_host/blob/master/Information_entropy_1.jpg?raw=true)
* 在p=1和p=0的情况下,不确定度为0,信息熵为0
* p=1/n(互斥时间中为0.5),不确定度最大,信息熵最大,故 $0\le H(p)\le logn$
* 联合事件的信息熵大于各个独立事件的信息熵之和: $H(x,y) \le H(x)+H(y)$
* $H(y) \ge H(y/x)$

举个栗子: 一道选择题有ABCD四个答案,用二进制编码需要的长度就是取2为底的对数：$log_2(4)=2$.
2个bit就可以对答案进行编码了(00,01,10,11). 这里其实隐含了一种假设:就是答题人完全不会做这道选择题,
四个答案对于他来说概率是均等的,均为p=1/4.所以,ABCD每个选项信息量为:
$$log_2(4)=log_2(\frac{1}{1/4})=-log_2(1/4)=-log_2(p)=2$$
信息熵为:
$$H(x)=-\sum_{x\in\{A,B,C,D\}}p_X(x)log[p_X(x)]=E[I_X(x)]=2$$
因为ABCD出现的概率均为p=1/4，所以上面式子算出来结果刚好是2. 从这个角度，信息熵就是对每个可能性编码需要长度的期望值。

如果答题人接受到一些其他人的信息对选择题的判断发生变化:
假设A出现的概率不变仍为1/4，C出现的概率变成了1/2，B和D则分别是1/8：P(A)=1/4，P(B)=1/8，P(C\)=1/2，P(D)=1/8。
信息熵发生变化:
$$H(x)=-\sum_{x\in\{A,B,C,D\}}p_X(x)log[p_X(x)]=\frac{1}{4}\times2+\frac{1}{4}\times2+\frac{1}{8}\times3+\frac{1}{2}\times1+\frac{1}{8}\times3=1.75$$
信息熵减小,说明均匀分布的信息熵确为最大

从编码的角度来看,C出现的次数(概率)最高，所以可以为C用短一些的编码，而出现次数较少的B和D则可以考虑用长一些的编码。这样的话，平均下来，对于一定的信息总量，需要的编码总长度就会少一些。
A:10, B:110, C:0, D:111, 对照熵的公式来计算一下编码长度的期望值，也就是平均编码长度.

再详细点，有200道选择题, 这200个答案中ABCD出现的次数恰好都严格和其出现概率成比例,也就是A：50次，B：25次，C：100次，D：25次。所以传递200个答案一共需要的bit数是:
$$50\times2+25\times3+100\times1+25\times3=350$$
那么平均下来每个答案耗费了350/200=1.75个bit编码长度。

### 1.3 条件熵
我们首先知道信息熵是考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。
条件熵的定义是: **X 给定的条件下 Y 的条件概率分布的熵对 X 的数学期望.**

$$\begin{align}
H(Y|X)&=\sum_{x}p(x)H(Y|X=x)\\
      &=-\sum_{x}p(x)\sum_{y}p(y|x)logp(y|x)\\
      &=-\sum_{x}\sum_{y}p(x,y)logp(y|x)\\
      &=-\sum_{x,y}p(x,y)logp(y|x)
\end{align}$$

**注意**: 这个条件熵，不是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少，变量的不确定性是多少,
因为条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的期望。

条件熵 H(Y|X)相当于联合熵 H(X,Y)减去单独的熵 H(X)，即H(Y|X)=H(X,Y)−H(X)，证明如下:
$$\begin{align}
H(X,Y)&=-\sum_{x,y}p(x,y)logp(x,y)\\
      &=-\sum_{x,y}p(x,y)log(p(y|x)p(x))\\
      &=-\sum_{x,y}p(x,y)logp(y|x) - \sum_{x,y}p(x,y)logp(x)\\
      &=H(Y|X)-\sum_{x}\sum_{y}p(x,y)logp(x)\\
      &=H(Y|X)-\sum_{y}p(x,y)\sum_{x}logp(x)\\
      &=H(Y|X)-\sum_{x}p(x)logp(x)\\
      &=H(Y|X)+H(X)
\end{align}$$

举个例子:

![avatar](https://github.com/superfishie/img_host/blob/master/entropy_example.png?raw=true)

随机变量 $Y=\{嫁,不嫁\}$, 可以统计出, 嫁和不嫁的概率都为6/12=0.5
$$H(Y)=-0.5log0.5-0.5log0.5=1$$
引入变量 $X=\{帅,不帅\}$
|Y\X| 帅|不帅|
|---|---|---|
|嫁  |3 |3|
|不嫁|5|1|

$$\begin{align}
P(Y=嫁,X=不帅)&=1/4\\
P(Y=嫁,X=帅)&=1/4\\
P(Y=不嫁,X=不帅)&=1/12\\
P(Y=不嫁,X=帅)&=5/12\\
P(Y=嫁|X=不帅)&=3/4\\
P(Y=嫁|X=帅)&=3/8\\
P(Y=不嫁|X=不帅)&=1/4\\
P(Y=不嫁|X=帅)&=5/8\\
P(X=帅)=8/12\\
P(X=不帅)=4/12\\
\\

H(Y|X)&=-\sum_{x,y}p(x,y)logp(y|x)\\
      &=-P(Y=嫁,X=帅)log(P(Y=嫁|X=帅))-P(Y=不嫁,X=帅)log(P(Y=不嫁|X=帅))-P(Y=嫁,X=不帅)log(P(Y=嫁|X=不帅))-P(Y=不嫁,X=不帅)log(P(Y=不嫁|X=不帅))\\
      &=-0.25*log(\frac{3}{8})-\frac{5}{12}*log(\frac{5}{8})-0.25*log(\frac{3}{4})-1/12*log(\frac{1}{4})\\
      &=0.6363+0.2704\\
      &=0.9067
\\
H(Y|X)&=\sum_{x}p(x)H(Y|X=x)\\
      &=-\sum_{x}p(x)\sum_{y}p(y|x)logp(y|x)\\
      &=P(X=帅)H(Y|X=帅)+P(X=不帅)H(Y|X=不帅)\\
      &=-P(X=帅)[P(Y=嫁|X=帅)log(P(Y=嫁|X=帅))+P(Y=不嫁|X=帅)log(P(Y=不嫁|X=帅))]-P(X=不帅)[P(Y=嫁|X=不帅)log(P(Y=嫁|X=不帅))+P(Y=不嫁|X=不帅)log(P(Y=不嫁|X=不帅))]\\
      &=\frac{8}{12}[-\frac{3}{8}\times log(\frac{3}{8})-\frac{5}{8}\times log(\frac{5}{8})]+\frac{4}{12}[-\frac{3}{4}\times log(\frac{3}{4})-\frac{1}{4}\times log(\frac{1}{4})]\\
      &=0.9067
\\
\end{align}$$

可以看出,帅或不帅对于嫁或不嫁的判断是有帮助的: 长的帅的人反而不容易找对象!!! 在得知了长相之后的条件熵H(Y|X)较H(Y)是减小的.
**我们用另一个变量对原变量分类后，原变量的不确定性就会减小了,不确定程度减少了多少就是信息的增益**

信息增益 = 熵 – 条件熵 = H(Y) - H(Y|X) = 1-0.9067=0.0933

信息增益的应用： 我们在利用进行分类的时候，常常选用信息增益更大的特征，信息增益大的特征对分类来说更加重要。
决策树就是通过信息增益来构造的，信息增益大的特征往往被构造成底层的节点。

### 1.4 互信息/信息增益(Mutual Information)
**定义**: 一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性
**公式**: $I(X;Y)=H(X)-H(X|Y)$
**性质**:
* $I(X;Y) \ge 0$
* $I(X;Y)=I(Y;X)$
* 假设X,Y完全无关，H(X) = H(X|Y) , 那么I(X;Y) = 0
* 假设X,Y完全相关，H(X|Y) =0， 那么I(X;Y) = H(X)
* 条件熵越大，互信息越小，条件熵越小，互信息越大。
![avatar](https://github.com/superfishie/img_host/blob/master/mutual_Info.jpg?raw=true)
* 条件熵和联合熵的关系:
$$\begin{align}
I(X;Y)&=H(X)-H(X|Y)\\
      &=H(Y)-H(Y|X)\\
      &=H(X)+H(Y)-H(X,Y)\\
      &=H(X,Y)-H(X|Y)-H(Y|X)
\end{align}$$

* 互信息的计算:
$$\begin{align}
I(X;Y)&=H(X)-H(X|Y)\\
      &=H(X)-[H(X,Y)-H(Y)]\\
      &=H(X)+H(Y)-H(X,Y)\\
      &=\sum_{x,y}p(x,y)log(p(x,y))-[\sum_{x}p(x)logp(x)+\sum_{y}p(x)logp(y)]\\
      &=\sum_{x,y}p(x,y)log(p(x,y))-[\sum_{x}\sum_{y}p(x,y)logp(x)+\sum_{y}\sum_{x}p(x,y)logp(y)]\\
      &=\sum_{x,y}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})
\end{align}$$

* 关于信息增益/互信息的直观理解
1. 我们知道存在两个随机事件 X,Y ，其中一个随机事件 X 给我们带来了一些不确定性 H(X) ，我们想衡量 Y,X 之间的关系。那么，如果 X,Y存在关联，当 Y 已知时， X 给我们的不确定性会变化，这个变化值就是 X 的信息熵减去当已知 Y 时， X 的条件熵，就是互信息。

2. 原来我对X有些不确定(不确定性为H(X))，告诉我Y后我对X不确定性变为H(X|Y), 这个不确定性的减少量就是X,Y之间的互信息I(X,Y)=H(X)-H(X|Y)。
更直观的意义可以理解为，当你完整的学到Y的所有知识的时候，你对X的知识的增长量就是I(X,Y)。
（相信我，每当你学到任何关于X的知识，都其实只是Y。没有人可以做到把一个学科彻底的（信息的完整性），完全正确的（信息传递过程中混入的噪声）学习）。

通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。
在数据集里，就是最大化两个数据集合所拟合出的概率分布的相关性。
当两个随机变量相同时,互信息最大，如下:
$$I(X;X)=H(X)-H(X|X)=H(X)$$
**在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。**

### 1.5 基尼指数
在CART算法中, 基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。
当一个节点中所有样本都是一个类时，基尼不纯度为零。
在分类问题中,假设有K个类,样本点属于第k类的概率为 $p_k$, 则概率分布的基尼指数定义为:
$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$
对于二分类问题,若样本点属于第一个类的概率是p,则概率分布的基尼指数为:
$$Gini(p)=2p(1-p)$$
对于给定的样本集合D,其基尼指数为:
$$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$$
这里, $C_k$是$D$中属于第$k$类的样本子集, $|C_k|$ 是样本子集中样本的个数, $|D|$ 是总样本个数, $K$是类的个数.
如果样本集合$D$根据特征A是否取某一可能值$a$被分割成$D_1$和$D_2$两部分,则在特征A的条件下,集合$D$的基尼指数定义为:
$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$
基尼指数$Gini(D)$ 表示集合D的不确定性,基尼指数$Gini(D,A)$ 表示经$A=a$分割后集合$D$的不确定性.基尼指数值越大,样本集合不确定性也就越大.
这和利用信息增益选特征的方式很像.

举个栗子: 计算长相和性格这两个特征的基尼指数,选择最优特征以及最优切分点

|长相| 帅|不帅|
|---|---|---|
|嫁  |3 |3|
|不嫁|5|1|

|性格| 不好|好|爆好|
|---|---|---|---|
|嫁  |1 |3|2|
|不嫁|3|3|0|
样本集合D中包含6个嫁,6个不嫁:
$$Gini(D)=2p(1-p)=0.5$$
根据长相帅不帅把D分割成了$D_1(帅)$ 和 $D_2(不帅)$,由于只有一个切分点,所以他们就是最优切分点.
$$Gini(D,A_1=帅)=\frac{8}{12}\times(2\times\frac{2}{5}\times\frac{3}{5})+\frac{4}{12}\times(2\times\frac{1}{4}\times\frac{3}{4})=0.445$$
根据性格好不好把D分成了三个子集:
$$Gini(D,A_2=不好)=\frac{4}{12}*(2*\frac{1}{4}*\frac{3}{4})+\frac{8}{12}*(2*\frac{3}{8}*\frac{5}{8})=0.4375$$
$$Gini(D,A_2=好)=\frac{1}{2}*(2*\frac{1}{2}*\frac{1}{2})+\frac{1}{2}*(2*\frac{1}{2}*\frac{1}{2})=0.5$$
$$Gini(D,A_2=爆好)=\frac{1}{6}*(2*1*0)+\frac{5}{6}*(2*\frac{3}{5}*\frac{2}{5})=0.4$$
所以$A_2=爆好$为$A_2$的最优切分点
